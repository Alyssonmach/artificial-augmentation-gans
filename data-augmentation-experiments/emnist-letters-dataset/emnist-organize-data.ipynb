{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing the EMNIST Letters Image Set\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Auxiliary Packages\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import EMNIST\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 124800\n",
       "    Root location: .\n",
       "    Split: Train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMNIST('.', download = True, split = 'letters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizing Images in Directories\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Data Dimension ==\n",
      "\n",
      "Train Images Shape: (124800, 28, 28)\n",
      "Train Labels Shape: (124800,)\n",
      "Test Images Shape: (20800, 28, 28)\n",
      "Test Images Shape: (20800,)\n"
     ]
    }
   ],
   "source": [
    "train_images_files = 'EMNIST/raw/emnist-letters-train-images-idx3-ubyte'\n",
    "train_labels_files = 'EMNIST/raw/emnist-letters-train-labels-idx1-ubyte'\n",
    "test_images_files = 'EMNIST/raw/emnist-letters-test-images-idx3-ubyte'\n",
    "test_labels_files = 'EMNIST/raw/emnist-letters-test-labels-idx1-ubyte'\n",
    "\n",
    "train_images_array = idx2numpy.convert_from_file(train_images_files)\n",
    "train_labels_array = idx2numpy.convert_from_file(train_labels_files)\n",
    "test_images_array = idx2numpy.convert_from_file(test_images_files)\n",
    "test_labels_array = idx2numpy.convert_from_file(test_labels_files)\n",
    "\n",
    "print('== Data Dimension ==\\n')\n",
    "print(f'Train Images Shape: {train_images_array.shape}')\n",
    "print(f'Train Labels Shape: {train_labels_array.shape}')\n",
    "print(f'Test Images Shape: {test_images_array.shape}')\n",
    "print(f'Test Images Shape: {test_labels_array.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_arrays(labels_array, extract):\n",
    "    '''\n",
    "    analyzes the amount of images contained in the array.\n",
    "\n",
    "    Input:\n",
    "        labels_array (array) --> array with all image labels.\n",
    "        extract (string) --> defines the analyzed image extract type label.\n",
    "    '''\n",
    "    \n",
    "    count, sum = 0, 0\n",
    "    for num in range(1, 27):\n",
    "        for label in labels_array:\n",
    "            if num == label: count += 1\n",
    "\n",
    "        print(f'{extract} images class {num}: {count} images')\n",
    "        sum += count\n",
    "        count = 0\n",
    "\n",
    "    print(f'\\n== {extract} total images: {sum} images ==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images class 1: 4800 images\n",
      "Train images class 2: 4800 images\n",
      "Train images class 3: 4800 images\n",
      "Train images class 4: 4800 images\n",
      "Train images class 5: 4800 images\n",
      "Train images class 6: 4800 images\n",
      "Train images class 7: 4800 images\n",
      "Train images class 8: 4800 images\n",
      "Train images class 9: 4800 images\n",
      "Train images class 10: 4800 images\n",
      "Train images class 11: 4800 images\n",
      "Train images class 12: 4800 images\n",
      "Train images class 13: 4800 images\n",
      "Train images class 14: 4800 images\n",
      "Train images class 15: 4800 images\n",
      "Train images class 16: 4800 images\n",
      "Train images class 17: 4800 images\n",
      "Train images class 18: 4800 images\n",
      "Train images class 19: 4800 images\n",
      "Train images class 20: 4800 images\n",
      "Train images class 21: 4800 images\n",
      "Train images class 22: 4800 images\n",
      "Train images class 23: 4800 images\n",
      "Train images class 24: 4800 images\n",
      "Train images class 25: 4800 images\n",
      "Train images class 26: 4800 images\n",
      "\n",
      "== Train total images: 124800 images ==\n"
     ]
    }
   ],
   "source": [
    "analyze_arrays(train_labels_array, 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images class 1: 800 images\n",
      "Test images class 2: 800 images\n",
      "Test images class 3: 800 images\n",
      "Test images class 4: 800 images\n",
      "Test images class 5: 800 images\n",
      "Test images class 6: 800 images\n",
      "Test images class 7: 800 images\n",
      "Test images class 8: 800 images\n",
      "Test images class 9: 800 images\n",
      "Test images class 10: 800 images\n",
      "Test images class 11: 800 images\n",
      "Test images class 12: 800 images\n",
      "Test images class 13: 800 images\n",
      "Test images class 14: 800 images\n",
      "Test images class 15: 800 images\n",
      "Test images class 16: 800 images\n",
      "Test images class 17: 800 images\n",
      "Test images class 18: 800 images\n",
      "Test images class 19: 800 images\n",
      "Test images class 20: 800 images\n",
      "Test images class 21: 800 images\n",
      "Test images class 22: 800 images\n",
      "Test images class 23: 800 images\n",
      "Test images class 24: 800 images\n",
      "Test images class 25: 800 images\n",
      "Test images class 26: 800 images\n",
      "\n",
      "== Test total images: 20800 images ==\n"
     ]
    }
   ],
   "source": [
    "analyze_arrays(test_labels_array, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_png(image_array, image_dir):\n",
    "    '''\n",
    "    saves an image array in png format.\n",
    "\n",
    "    Input:\n",
    "        image_array (array) --> image array\n",
    "        image_dir (str) --> string with directory and image name.png\n",
    "    Returns:\n",
    "    '''\n",
    "    \n",
    "    rotated_image = np.rot90(image_array, k=-1)\n",
    "    flipped_image = np.fliplr(rotated_image)\n",
    "\n",
    "    image = Image.fromarray(flipped_image)\n",
    "    image.save(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories(main_directory, sub_directories):\n",
    "    '''\n",
    "    creates directories to store the dataset images.\n",
    "\n",
    "    Input:\n",
    "        main_directory (string) --> main folder name.\n",
    "        sub_directories (string) --> name of subdirectories.\n",
    "    '''\n",
    "\n",
    "    os.makedirs(main_directory, exist_ok = True)\n",
    "\n",
    "    for sub in sub_directories:\n",
    "        sub_directory = os.path.join(main_directory, sub)\n",
    "        os.makedirs(sub_directory, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_png_all_images(directory, sub_dir, labels_array, images_array):\n",
    "    '''\n",
    "    assemble a directory of images in png format from array lists.\n",
    "\n",
    "    Input:\n",
    "        directory (string) --> main directory.\n",
    "        sub_dir (string) --> sub directories indexed in order with classes.\n",
    "        labels_array (array) --> array list containing all labels.\n",
    "        images_array (array) --> array list containing all images. \n",
    "    '''\n",
    "\n",
    "    for num in range(1, 27):\n",
    "        for index, label in enumerate(labels_array):\n",
    "            if num == label: \n",
    "                array_to_png(images_array[index], \n",
    "                             directory + sub_dir[num - 1] + f'/{index}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_directories = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                   'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "make_directories('EMNIST/original-data/train', sub_directories)\n",
    "make_directories('EMNIST/original-data/test', sub_directories)\n",
    "\n",
    "array_to_png_all_images('EMNIST/original-data/train/', sub_directories, train_labels_array, train_images_array)\n",
    "array_to_png_all_images('EMNIST/original-data/test/', sub_directories, test_labels_array, test_images_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_directory(directory, extract):\n",
    "    '''\n",
    "    analyzes the amount of images present in subdirectories.\n",
    "\n",
    "    Input:\n",
    "        directory (string) --> main directory to be parsed.\n",
    "        extract (string) --> data extract type.\n",
    "    '''\n",
    "\n",
    "    count, sum = 0, 0\n",
    "    for _, label in enumerate(os.listdir(directory)):\n",
    "        count = len(os.listdir(directory + label))\n",
    "        print(f'{extract} images in \"{label}\" directory: {count} images')\n",
    "        sum += count \n",
    "        count = 0\n",
    "\n",
    "    print(f'\\n== {extract} total images: {sum} images ==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images in \"a\" directory: 4800 images\n",
      "Train images in \"b\" directory: 4800 images\n",
      "Train images in \"c\" directory: 4800 images\n",
      "Train images in \"d\" directory: 4800 images\n",
      "Train images in \"e\" directory: 4800 images\n",
      "Train images in \"f\" directory: 4800 images\n",
      "Train images in \"g\" directory: 4800 images\n",
      "Train images in \"h\" directory: 4800 images\n",
      "Train images in \"i\" directory: 4800 images\n",
      "Train images in \"j\" directory: 4800 images\n",
      "Train images in \"k\" directory: 4800 images\n",
      "Train images in \"l\" directory: 4800 images\n",
      "Train images in \"m\" directory: 4800 images\n",
      "Train images in \"n\" directory: 4800 images\n",
      "Train images in \"o\" directory: 4800 images\n",
      "Train images in \"p\" directory: 4800 images\n",
      "Train images in \"q\" directory: 4800 images\n",
      "Train images in \"r\" directory: 4800 images\n",
      "Train images in \"s\" directory: 4800 images\n",
      "Train images in \"t\" directory: 4800 images\n",
      "Train images in \"u\" directory: 4800 images\n",
      "Train images in \"v\" directory: 4800 images\n",
      "Train images in \"w\" directory: 4800 images\n",
      "Train images in \"x\" directory: 4800 images\n",
      "Train images in \"y\" directory: 4800 images\n",
      "Train images in \"z\" directory: 4800 images\n",
      "\n",
      "== Train total images: 124800 images ==\n"
     ]
    }
   ],
   "source": [
    "analyze_directory('EMNIST/original-data/train/', 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images in \"a\" directory: 800 images\n",
      "Test images in \"b\" directory: 800 images\n",
      "Test images in \"c\" directory: 800 images\n",
      "Test images in \"d\" directory: 800 images\n",
      "Test images in \"e\" directory: 800 images\n",
      "Test images in \"f\" directory: 800 images\n",
      "Test images in \"g\" directory: 800 images\n",
      "Test images in \"h\" directory: 800 images\n",
      "Test images in \"i\" directory: 800 images\n",
      "Test images in \"j\" directory: 800 images\n",
      "Test images in \"k\" directory: 800 images\n",
      "Test images in \"l\" directory: 800 images\n",
      "Test images in \"m\" directory: 800 images\n",
      "Test images in \"n\" directory: 800 images\n",
      "Test images in \"o\" directory: 800 images\n",
      "Test images in \"p\" directory: 800 images\n",
      "Test images in \"q\" directory: 800 images\n",
      "Test images in \"r\" directory: 800 images\n",
      "Test images in \"s\" directory: 800 images\n",
      "Test images in \"t\" directory: 800 images\n",
      "Test images in \"u\" directory: 800 images\n",
      "Test images in \"v\" directory: 800 images\n",
      "Test images in \"w\" directory: 800 images\n",
      "Test images in \"x\" directory: 800 images\n",
      "Test images in \"y\" directory: 800 images\n",
      "Test images in \"z\" directory: 800 images\n",
      "\n",
      "== Test total images: 20800 images ==\n"
     ]
    }
   ],
   "source": [
    "analyze_directory('EMNIST/original-data/test/', 'Test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizing the Data to Store the Artificial Images\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_directories = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                   'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "make_directories('EMNIST/artificial-augmentation/train', sub_directories)\n",
    "make_directories('EMNIST/artificial-augmentation/test', sub_directories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
